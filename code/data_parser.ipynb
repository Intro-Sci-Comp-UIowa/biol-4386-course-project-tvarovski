{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Sani.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TqrXHlb8mmOV"
      },
      "source": [
        "# Variant Analysis\n",
        "\n",
        "This Jupyter notebook is designed for filtration of common variants and analysis as outlibed by the steps in my [github repository](https://github.com/Intro-Sci-Comp-UIowa/biol-4386-course-project-tvarovski). This notebook will be using spark distributred computing environment for faster computation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wXGbbaHq8tsq"
      },
      "source": [
        "## Installing Dependencies and Importing Libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m1VMVSpOmKlJ"
      },
      "source": [
        "!apt-get update\n",
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "!wget -q https://www-us.apache.org/dist/spark/spark-2.4.7/spark-2.4.7-bin-hadoop2.7.tgz\n",
        "!tar xf spark-2.4.7-bin-hadoop2.7.tgz\n",
        "!pip install -q findspark\n",
        "\n",
        "import os\n",
        "import findspark\n",
        "\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-2.4.7-bin-hadoop2.7\"\n",
        "findspark.init()\n",
        "\n",
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder.appName(\"CLC_mutations\").getOrCreate()\n",
        "\n",
        "#!pip install scikit-allel[full]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O39d6UPMmQn2"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import functools\n",
        "from pyspark.sql.functions import lit, udf, struct\n",
        "from pyspark.sql.types import StringType\n",
        "#import allel"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cktC6SCcmfJu"
      },
      "source": [
        "## Loading Datasets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-Conwr2xmT2J"
      },
      "source": [
        "# If the vxf file is not converted into a a \"normal\" table do it here, but better \n",
        "# to do it via GATK's tools if possible\n",
        "\n",
        "fields=['CHROM', 'POS', 'DP','AD', 'AF', 'REF', 'ALT','INFO', 'ID']\n",
        "\n",
        "allel.vcf_to_csv('/content/SRR4047707_mutect.vcf', 'mutect_07.csv', fields=fields)\n",
        "allel.vcf_to_csv('/content/SRR4047717_mutect.vcf', 'mutect_17.csv', fields=fields)\n",
        "allel.vcf_to_csv('/content/SRR4047722_mutect.vcf', 'mutect_22.csv', fields=fields)\n",
        "allel.vcf_to_csv('/content/SRR4047723_mutect.vcf', 'mutect_23.csv', fields=fields)\n",
        "\n",
        "#allel.vcf_to_csv('.vcf', 'haplotype_07.csv', fields=fields)\n",
        "#allel.vcf_to_csv('.vcf', 'haplotype_15.csv', fields=fields)\n",
        "#allel.vcf_to_csv('.vcf', 'haplotype_17.csv', fields=fields)\n",
        "#allel.vcf_to_csv('.vcf', 'haplotype_22.csv', fields=fields)\n",
        "#allel.vcf_to_csv('.vcf', 'haplotype_23.csv', fields=fields)\n"
      ],
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3UBDN509HW2Z"
      },
      "source": [
        "Load csv files into Data Frames; insert DF objects to lists"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E-GHKh8mHefU"
      },
      "source": [
        "df_mutect_07 = spark.read.csv('out07.tsv', header=True, inferSchema=True, sep='\\t')\n",
        "df_mutect_17 = spark.read.csv('out17.tsv', header=True, inferSchema=True, sep='\\t')\n",
        "df_mutect_22 = spark.read.csv('out22.tsv', header=True, inferSchema=True, sep='\\t')\n",
        "df_mutect_23 = spark.read.csv('out23.tsv', header=True, inferSchema=True, sep='\\t')\n",
        "\n",
        "mutect_df_list = [df_mutect_07, df_mutect_17, df_mutect_22, df_mutect_23] \n",
        "\n",
        "#df_haplotype_07 = spark.read.csv('haplotype_07.csv', header=True, inferSchema=True)\n",
        "#df_haplotype_15 = spark.read.csv('haplotype_15.csv', header=True, inferSchema=True)\n",
        "#df_haplotype_17 = spark.read.csv('haplotype_17.csv', header=True, inferSchema=True)\n",
        "#df_haplotype_22 = spark.read.csv('haplotype_22.csv', header=True, inferSchema=True)\n",
        "#df_haplotype_23 = spark.read.csv('haplotype_23.csv', header=True, inferSchema=True)\n",
        "\n",
        "#haplotype_df_list = [df_haplotype_07, df_haplotype_15, df_haplotype_17, df_haplotype_22, df_haplotype_23]"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ywele90Ana47"
      },
      "source": [
        "#describing All Variant Stats to get an idea what we have\n",
        "for i in mutect_df_list:\n",
        "  #first show first few rows\n",
        "  i.show()\n",
        "  #describe and show basic stats\n",
        "  i.describe().show()\n",
        "\n",
        "#for i in haplotype_df_list:\n",
        "  #i.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c8lFTmunuAhd"
      },
      "source": [
        "## Preparing DataFrames for Filtering"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FjPZSBoM-8om"
      },
      "source": [
        "Select and Rename DataFrame Columns"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "52sUZ-IPnkyr"
      },
      "source": [
        "# cut out the normal columns (I think they are not relevant)\n",
        "remove_normal_name = lambda col: 'h4.lib1' in col\n",
        "\n",
        "# rename sample columns so that they are consistent across all samples\n",
        "renaming_AF = lambda col: 'AF' in col\n",
        "renaming_AD = lambda col: 'AD' in col\n",
        "\n",
        "for i in range(len(mutect_df_list)):\n",
        "  mutect_df_list[i] = mutect_df_list[i].drop(*filter(remove_normal_name, mutect_df_list[i].columns))\n",
        "  mutect_df_list[i] = mutect_df_list[i].withColumnRenamed(*filter(renaming_AF, mutect_df_list[i].columns), 'AF')\n",
        "  mutect_df_list[i] = mutect_df_list[i].withColumnRenamed(*filter(renaming_AD, mutect_df_list[i].columns), 'AD')\n",
        "\n",
        "#for i in range(len(haplotype_df_list)):\n",
        "#  haplotype_df_list[i] = haplotype_df_list[i].withColumnRenamed(*filter(renaming_AF, haplotype_df_list[i].columns), 'AF')\n",
        "#  haplotype_df_list[i] = haplotype_df_list[i].withColumnRenamed(*filter(renaming_AD, haplotype_df_list[i].columns), 'AD')"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gXqhVk9c-bJc"
      },
      "source": [
        "#describing All Variant Stats to check if renaming worked\n",
        "\n",
        "for i in mutect_df_list:\n",
        "  #first show first few rows\n",
        "  i.show()\n",
        "\n",
        "#for i in haplotype_df_list:\n",
        "  #i.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t_bn4VX4nt6U"
      },
      "source": [
        "## Quality and SNP Filtering"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_F94PyGLnvw9"
      },
      "source": [
        "for i in range(len(mutect_df_list)):\n",
        "  mutect_df_list[i] = mutect_df_list[i].filter(\n",
        "               mutect_df_list[i].TYPE == \"SNP\").filter(\n",
        "               (mutect_df_list[i].AF >= 0.45) & (mutect_df_list[i].AF <= 0.55) | (mutect_df_list[i].AF >= 0.90))\n",
        "\n",
        "#for i in range(len(haplotype_df_list)):\n",
        "#  haplotype_df_list[i] = haplotype_df_list[i].filter(\n",
        "#               haplotype_df_list[i].TYPE == \"SNP\").filter(\n",
        "#               (haplotype_df_list[i].AF >= 0.45) & (haplotype_df_list[i].AF <= 0.55) | (haplotype_df_list[i].AF >= 0.90))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Y_J_ajpoA1R"
      },
      "source": [
        "## Subtraction of Common Variants Between Samples\n",
        "\n",
        "Maybe should do it by position only?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hmTzg3gVn_Yb"
      },
      "source": [
        "# Working on this, it will be usefull for the HaplotypeCaller Data\n",
        "subtracted_df_list = []\n",
        "normal = df_haplotype_15 # this is the blood sample\n",
        "\n",
        "for df in haplotype_df_list:\n",
        "# this will subtract all common rows between normal/control and experimental\n",
        "# sample. I might need to revist to filter variants by position only not all columns\n",
        "  if df != normal:\n",
        "    temp_df = temp_df.subtract(j) \n",
        "\n",
        "  subtracted_df_list.append(temp_df)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DO2A_PnSBQa8"
      },
      "source": [
        "## Preparing the Data for plotting"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TqLmUPhKBNo4"
      },
      "source": [
        "# Create a user define function (UDF) to work on multiple columns to extract \n",
        "# the mutation spectra information\n",
        "\n",
        "#define a function with logic for the mutation type assignment\n",
        "def findType(colA, colB):\n",
        "  if ((colA == 'C') & (colB == 'T') | (colA == 'G') & (colB == 'A')):\n",
        "    return('C_to_T')\n",
        "  if ((colA == 'C') & (colB == 'A') | (colA == 'G') & (colB == 'T')):\n",
        "    return('C_to_A')\n",
        "  if ((colA == 'C') & (colB == 'G') | (colA == 'G') & (colB == 'C')):\n",
        "    return('C_to_G')\n",
        "  if ((colA == 'T') & (colB == 'C') | (colA == 'A') & (colB == 'G')):\n",
        "    return('T_to_C')\n",
        "  if ((colA == 'T') & (colB == 'G') | (colA == 'A') & (colB == 'C')):\n",
        "    return('T_to_G')\n",
        "  if ((colA == 'T') & (colB == 'A') | (colA == 'A') & (colB == 'T')):\n",
        "    return('T_to_A')\n",
        "\n",
        "# apply pySpark's udf to the custom function \n",
        "createType = udf(findType, StringType())\n",
        "\n",
        "#add a column containing the output of the udf\n",
        "for i in range(len(mutect_df_list)):\n",
        "  mutect_df_list[i] = mutect_df_list[i].withColumn('Type', createType(mutect_df_list[i].REF,mutect_df_list[i].ALT))"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wYborCIAHrcD"
      },
      "source": [
        "#check if the adding columns worked\n",
        "for i in mutect_df_list:\n",
        "  i.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "karoXyN_YESL"
      },
      "source": [
        "These columns now can be taken and used for plotting in excel (doing stacked percent bar charts in python is weirdly difficult)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OuvFQSrGH3qT"
      },
      "source": [
        "# Count all occurences of each type of mutation and \n",
        "# display the resulting summary table for each DF\n",
        "for i in mutect_df_list:\n",
        "  i.groupBy(\"Type\").count().show()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}