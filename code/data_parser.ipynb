{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Sani.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TqrXHlb8mmOV"
      },
      "source": [
        "# Variant Analysis\n",
        "\n",
        "This Jupyter notebook is designed for filtration of common variants and analysis as outlibed by the steps in my [github repository](https://github.com/Intro-Sci-Comp-UIowa/biol-4386-course-project-tvarovski). This notebook will be using spark distributred computing environment for faster computation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wXGbbaHq8tsq"
      },
      "source": [
        "## Installing Dependencies and Importing Libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "m1VMVSpOmKlJ",
        "outputId": "5d33293f-93a3-48bf-f9d0-53b4ab3208f6"
      },
      "source": [
        "!apt-get update\n",
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "!wget -q https://www-us.apache.org/dist/spark/spark-2.4.7/spark-2.4.7-bin-hadoop2.7.tgz\n",
        "!tar xf spark-2.4.7-bin-hadoop2.7.tgz\n",
        "!pip install -q findspark\n",
        "\n",
        "import os\n",
        "import findspark\n",
        "\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-2.4.7-bin-hadoop2.7\"\n",
        "findspark.init()\n",
        "\n",
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder.appName(\"CLC_mutations\").getOrCreate()\n",
        "\n",
        "#!pip install scikit-allel[full]"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Get:1 http://security.ubuntu.com/ubuntu bionic-security InRelease [88.7 kB]\n",
            "Get:2 https://cloud.r-project.org/bin/linux/ubuntu bionic-cran40/ InRelease [3,626 B]\n",
            "Ign:3 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  InRelease\n",
            "Get:4 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic InRelease [15.9 kB]\n",
            "Hit:5 http://archive.ubuntu.com/ubuntu bionic InRelease\n",
            "Ign:6 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  InRelease\n",
            "Get:7 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  Release [697 B]\n",
            "Hit:8 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  Release\n",
            "Get:9 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  Release.gpg [801 B]\n",
            "Get:10 http://archive.ubuntu.com/ubuntu bionic-updates InRelease [88.7 kB]\n",
            "Hit:11 http://ppa.launchpad.net/cran/libgit2/ubuntu bionic InRelease\n",
            "Get:12 http://security.ubuntu.com/ubuntu bionic-security/restricted amd64 Packages [372 kB]\n",
            "Get:13 http://archive.ubuntu.com/ubuntu bionic-backports InRelease [74.6 kB]\n",
            "Get:14 http://security.ubuntu.com/ubuntu bionic-security/universe amd64 Packages [1,404 kB]\n",
            "Get:15 http://ppa.launchpad.net/deadsnakes/ppa/ubuntu bionic InRelease [15.9 kB]\n",
            "Get:16 http://security.ubuntu.com/ubuntu bionic-security/main amd64 Packages [2,077 kB]\n",
            "Hit:17 http://ppa.launchpad.net/graphics-drivers/ppa/ubuntu bionic InRelease\n",
            "Ign:19 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  Packages\n",
            "Get:19 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  Packages [634 kB]\n",
            "Get:20 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic/main Sources [1,751 kB]\n",
            "Get:21 http://archive.ubuntu.com/ubuntu bionic-updates/restricted amd64 Packages [402 kB]\n",
            "Get:22 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 Packages [2,175 kB]\n",
            "Get:23 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic/main amd64 Packages [896 kB]\n",
            "Get:24 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 Packages [2,508 kB]\n",
            "Get:25 http://ppa.launchpad.net/deadsnakes/ppa/ubuntu bionic/main amd64 Packages [39.5 kB]\n",
            "Fetched 12.5 MB in 4s (3,146 kB/s)\n",
            "Reading package lists... Done\n",
            "Collecting scikit-allel[full]\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/db/1d/bfdbb30eced4178d2c2677b94f78e26b856c9907cfb0e910fe4fd1811a9c/scikit-allel-1.3.3.tar.gz (10.8MB)\n",
            "\u001b[K     |████████████████████████████████| 10.8MB 3.3MB/s \n",
            "\u001b[?25hRequirement already satisfied: cython in /usr/local/lib/python3.7/dist-packages (from scikit-allel[full]) (0.29.22)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from scikit-allel[full]) (1.19.5)\n",
            "Requirement already satisfied: dask[array] in /usr/local/lib/python3.7/dist-packages (from scikit-allel[full]) (2.12.0)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from scikit-allel[full]) (1.4.1)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from scikit-allel[full]) (3.2.2)\n",
            "Requirement already satisfied: seaborn in /usr/local/lib/python3.7/dist-packages (from scikit-allel[full]) (0.11.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from scikit-allel[full]) (1.1.5)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from scikit-allel[full]) (0.22.2.post1)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.7/dist-packages (from scikit-allel[full]) (2.10.0)\n",
            "Requirement already satisfied: numexpr in /usr/local/lib/python3.7/dist-packages (from scikit-allel[full]) (2.7.3)\n",
            "Collecting bcolz\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/5c/4e/23942de9d5c0fb16f10335fa83e52b431bcb8c0d4a8419c9ac206268c279/bcolz-1.2.1.tar.gz (1.5MB)\n",
            "\u001b[K     |████████████████████████████████| 1.5MB 43.4MB/s \n",
            "\u001b[?25hCollecting zarr\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/93/ae/653da881ff4281d5d59b3901e13a2127182e8679db10b322365588d43111/zarr-2.7.0-py3-none-any.whl (137kB)\n",
            "\u001b[K     |████████████████████████████████| 143kB 56.3MB/s \n",
            "\u001b[?25hCollecting hmmlearn\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/4b/98/a2829aeb942b7146034d497afb3fc738a78a4fbd4797a039c19a94bb31f7/hmmlearn-0.2.5-cp37-cp37m-manylinux1_x86_64.whl (369kB)\n",
            "\u001b[K     |████████████████████████████████| 378kB 39.7MB/s \n",
            "\u001b[?25hCollecting pomegranate\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a7/65/98bbcc034daac7b584e82a0f91f140dbb9ef502a1b7c96f16f3deb847232/pomegranate-0.14.4-cp37-cp37m-manylinux2010_x86_64.whl (17.9MB)\n",
            "\u001b[K     |████████████████████████████████| 17.9MB 261kB/s \n",
            "\u001b[?25hCollecting nose\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/15/d8/dd071918c040f50fa1cf80da16423af51ff8ce4a0f2399b7bf8de45ac3d9/nose-1.3.7-py3-none-any.whl (154kB)\n",
            "\u001b[K     |████████████████████████████████| 163kB 44.5MB/s \n",
            "\u001b[?25hRequirement already satisfied: toolz>=0.7.3; extra == \"array\" in /usr/local/lib/python3.7/dist-packages (from dask[array]->scikit-allel[full]) (0.11.1)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->scikit-allel[full]) (1.3.1)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->scikit-allel[full]) (2.8.1)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib->scikit-allel[full]) (2.4.7)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib->scikit-allel[full]) (0.10.0)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas->scikit-allel[full]) (2018.9)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->scikit-allel[full]) (1.0.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from h5py->scikit-allel[full]) (1.15.0)\n",
            "Collecting asciitree\n",
            "  Downloading https://files.pythonhosted.org/packages/2d/6a/885bc91484e1aa8f618f6f0228d76d0e67000b0fdd6090673b777e311913/asciitree-0.3.3.tar.gz\n",
            "Collecting fasteners\n",
            "  Downloading https://files.pythonhosted.org/packages/78/20/c862d765287e9e8b29f826749ebae8775bdca50b2cb2ca079346d5fbfd76/fasteners-0.16-py2.py3-none-any.whl\n",
            "Collecting numcodecs>=0.6.4\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/e9/fa/a4450b9b6bac850d65316d00a1417e633fc0fe36d66cc6407640242cd8e4/numcodecs-0.7.3-cp37-cp37m-manylinux2010_x86_64.whl (5.8MB)\n",
            "\u001b[K     |████████████████████████████████| 5.8MB 35.0MB/s \n",
            "\u001b[?25hRequirement already satisfied: pyyaml in /usr/local/lib/python3.7/dist-packages (from pomegranate->scikit-allel[full]) (3.13)\n",
            "Requirement already satisfied: networkx>=2.0 in /usr/local/lib/python3.7/dist-packages (from pomegranate->scikit-allel[full]) (2.5.1)\n",
            "Requirement already satisfied: decorator<5,>=4.3 in /usr/local/lib/python3.7/dist-packages (from networkx>=2.0->pomegranate->scikit-allel[full]) (4.4.2)\n",
            "Building wheels for collected packages: scikit-allel, bcolz, asciitree\n",
            "  Building wheel for scikit-allel (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for scikit-allel: filename=scikit_allel-1.3.3-cp37-cp37m-linux_x86_64.whl size=4695963 sha256=b076775d4938960146195cab83b3faa601b15acfa7c69da9abaa9a90638690f4\n",
            "  Stored in directory: /root/.cache/pip/wheels/a7/2d/dc/a1ef6b48b94aa072114bbb8c81de4393adef3b595679aaa910\n",
            "  Building wheel for bcolz (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for bcolz: filename=bcolz-1.2.1-cp37-cp37m-linux_x86_64.whl size=2649508 sha256=c479d3c344a25f9b09583f6e8774741ec83a9feb147f2bcd1be6e5bd9ae16133\n",
            "  Stored in directory: /root/.cache/pip/wheels/9f/78/26/fb8c0acb91a100dc8914bf236c4eaa4b207cb876893c40b745\n",
            "  Building wheel for asciitree (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for asciitree: filename=asciitree-0.3.3-cp37-none-any.whl size=5037 sha256=83087f8d87926ed86a192ce32c784f709a2839e80974254dd69739f1fbfc83cc\n",
            "  Stored in directory: /root/.cache/pip/wheels/1d/d9/58/9808b306744df0208fccc640d3d9952a5bc7468502d42897d5\n",
            "Successfully built scikit-allel bcolz asciitree\n",
            "\u001b[31mERROR: pomegranate 0.14.4 has requirement numpy>=1.20.0, but you'll have numpy 1.19.5 which is incompatible.\u001b[0m\n",
            "Installing collected packages: bcolz, asciitree, fasteners, numcodecs, zarr, hmmlearn, pomegranate, nose, scikit-allel\n",
            "Successfully installed asciitree-0.3.3 bcolz-1.2.1 fasteners-0.16 hmmlearn-0.2.5 nose-1.3.7 numcodecs-0.7.3 pomegranate-0.14.4 scikit-allel-1.3.3 zarr-2.7.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O39d6UPMmQn2"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import functools\n",
        "from pyspark.sql.functions import lit, udf, struct\n",
        "from pyspark.sql.types import StringType\n",
        "import allel"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cktC6SCcmfJu"
      },
      "source": [
        "## Loading Datasets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-Conwr2xmT2J"
      },
      "source": [
        "# If the vxf file is not converted into a a \"normal\" table do it here, but better \n",
        "# to do it via GATK's tools if possible\n",
        "\n",
        "fields=['CHROM', 'POS', 'DP','AD', 'AF', 'REF', 'ALT','INFO', 'ID']\n",
        "\n",
        "allel.vcf_to_csv('/content/SRR4047707_mutect.vcf', 'mutect_07.csv', fields=fields)\n",
        "allel.vcf_to_csv('/content/SRR4047717_mutect.vcf', 'mutect_17.csv', fields=fields)\n",
        "allel.vcf_to_csv('/content/SRR4047722_mutect.vcf', 'mutect_22.csv', fields=fields)\n",
        "allel.vcf_to_csv('/content/SRR4047723_mutect.vcf', 'mutect_23.csv', fields=fields)\n",
        "\n",
        "#allel.vcf_to_csv('.vcf', 'haplotype_07.csv', fields=fields)\n",
        "#allel.vcf_to_csv('.vcf', 'haplotype_15.csv', fields=fields)\n",
        "#allel.vcf_to_csv('.vcf', 'haplotype_17.csv', fields=fields)\n",
        "#allel.vcf_to_csv('.vcf', 'haplotype_22.csv', fields=fields)\n",
        "#allel.vcf_to_csv('.vcf', 'haplotype_23.csv', fields=fields)\n"
      ],
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3UBDN509HW2Z"
      },
      "source": [
        "Load csv files into Data Frames; insert DF objects to lists"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E-GHKh8mHefU"
      },
      "source": [
        "df_mutect_07 = spark.read.csv('out07.tsv', header=True, inferSchema=True, sep='\\t')\n",
        "df_mutect_17 = spark.read.csv('out17.tsv', header=True, inferSchema=True, sep='\\t')\n",
        "df_mutect_22 = spark.read.csv('out22.tsv', header=True, inferSchema=True, sep='\\t')\n",
        "df_mutect_23 = spark.read.csv('out23.tsv', header=True, inferSchema=True, sep='\\t')\n",
        "\n",
        "mutect_df_list = [df_mutect_07, df_mutect_17, df_mutect_22, df_mutect_23] \n",
        "\n",
        "#df_haplotype_07 = spark.read.csv('haplotype_07.csv', header=True, inferSchema=True)\n",
        "#df_haplotype_15 = spark.read.csv('haplotype_15.csv', header=True, inferSchema=True)\n",
        "#df_haplotype_17 = spark.read.csv('haplotype_17.csv', header=True, inferSchema=True)\n",
        "#df_haplotype_22 = spark.read.csv('haplotype_22.csv', header=True, inferSchema=True)\n",
        "#df_haplotype_23 = spark.read.csv('haplotype_23.csv', header=True, inferSchema=True)\n",
        "\n",
        "#haplotype_df_list = [df_haplotype_07, df_haplotype_15, df_haplotype_17, df_haplotype_22, df_haplotype_23]"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ywele90Ana47"
      },
      "source": [
        "#describing All Variant Stats to get an idea what we have\n",
        "for i in mutect_df_list:\n",
        "  #first show first few rows\n",
        "  i.show()\n",
        "  #describe and show basic stats\n",
        "  i.describe().show()\n",
        "\n",
        "#for i in haplotype_df_list:\n",
        "  #i.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c8lFTmunuAhd"
      },
      "source": [
        "## Preparing DataFrames for Filtering"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FjPZSBoM-8om"
      },
      "source": [
        "Select and Rename DataFrame Columns"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "52sUZ-IPnkyr"
      },
      "source": [
        "# cut out the normal columns (I think they are not relevant)\n",
        "remove_normal_name = lambda col: 'h4.lib1' in col\n",
        "\n",
        "# rename sample columns so that they are consistent across all samples\n",
        "renaming_AF = lambda col: 'AF' in col\n",
        "renaming_AD = lambda col: 'AD' in col\n",
        "\n",
        "for i in range(len(mutect_df_list)):\n",
        "  mutect_df_list[i] = mutect_df_list[i].drop(*filter(remove_normal_name, mutect_df_list[i].columns))\n",
        "  mutect_df_list[i] = mutect_df_list[i].withColumnRenamed(*filter(renaming_AF, mutect_df_list[i].columns), 'AF')\n",
        "  mutect_df_list[i] = mutect_df_list[i].withColumnRenamed(*filter(renaming_AD, mutect_df_list[i].columns), 'AD')\n",
        "\n",
        "#for i in range(len(haplotype_df_list)):\n",
        "#  haplotype_df_list[i] = haplotype_df_list[i].withColumnRenamed(*filter(renaming_AF, haplotype_df_list[i].columns), 'AF')\n",
        "#  haplotype_df_list[i] = haplotype_df_list[i].withColumnRenamed(*filter(renaming_AD, haplotype_df_list[i].columns), 'AD')"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gXqhVk9c-bJc"
      },
      "source": [
        "#describing All Variant Stats to check if renaming worked\n",
        "\n",
        "for i in mutect_df_list:\n",
        "  #first show first few rows\n",
        "  i.show()\n",
        "\n",
        "#for i in haplotype_df_list:\n",
        "  #i.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t_bn4VX4nt6U"
      },
      "source": [
        "## Quality and SNP Filtering"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_F94PyGLnvw9"
      },
      "source": [
        "for i in range(len(mutect_df_list)):\n",
        "  mutect_df_list[i] = mutect_df_list[i].filter(\n",
        "               mutect_df_list[i].TYPE == \"SNP\").filter(\n",
        "               (mutect_df_list[i].AF >= 0.45) & (mutect_df_list[i].AF <= 0.55) | (mutect_df_list[i].AF >= 0.90))\n",
        "\n",
        "#for i in range(len(haplotype_df_list)):\n",
        "#  haplotype_df_list[i] = haplotype_df_list[i].filter(\n",
        "#               haplotype_df_list[i].TYPE == \"SNP\").filter(\n",
        "#               (haplotype_df_list[i].AF >= 0.45) & (haplotype_df_list[i].AF <= 0.55) | (haplotype_df_list[i].AF >= 0.90))\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Y_J_ajpoA1R"
      },
      "source": [
        "## Subtraction of Common Variants Between Samples\n",
        "\n",
        "Maybe should do it by position only?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hmTzg3gVn_Yb"
      },
      "source": [
        "# Working on this, it will be usefull for the HaplotypeCaller Data\n",
        "subtracted_df_list = []\n",
        "normal = df_haplotype_15 # this is the blood sample\n",
        "\n",
        "for df in haplotype_df_list:\n",
        "# this will subtract all common rows between normal/control and experimental\n",
        "# sample. I might need to revist to filter variants by position only not all columns\n",
        "  if df != normal:\n",
        "    temp_df = temp_df.subtract(j) \n",
        "\n",
        "  subtracted_df_list.append(temp_df)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DO2A_PnSBQa8"
      },
      "source": [
        "## Preparing the Data for plotting"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TqLmUPhKBNo4"
      },
      "source": [
        "# Create a user define function (UDF) to work on multiple columns to extract \n",
        "# the mutation spectra information\n",
        "\n",
        "#define a function with logic for the mutation type assignment\n",
        "def findType(colA, colB):\n",
        "  if ((colA == 'C') & (colB == 'T') | (colA == 'G') & (colB == 'A')):\n",
        "    return('C_to_T')\n",
        "  if ((colA == 'C') & (colB == 'A') | (colA == 'G') & (colB == 'T')):\n",
        "    return('C_to_A')\n",
        "  if ((colA == 'C') & (colB == 'G') | (colA == 'G') & (colB == 'C')):\n",
        "    return('C_to_G')\n",
        "  if ((colA == 'T') & (colB == 'C') | (colA == 'A') & (colB == 'G')):\n",
        "    return('T_to_C')\n",
        "  if ((colA == 'T') & (colB == 'G') | (colA == 'A') & (colB == 'C')):\n",
        "    return('T_to_G')\n",
        "  if ((colA == 'T') & (colB == 'A') | (colA == 'A') & (colB == 'T')):\n",
        "    return('T_to_A')\n",
        "\n",
        "# apply pySpark's udf to the custom function \n",
        "createType = udf(findType, StringType())\n",
        "\n",
        "#add a column containing the output of the udf\n",
        "for i in range(len(mutect_df_list)):\n",
        "  mutect_df_list[i] = mutect_df_list[i].withColumn('Type', createType(mutect_df_list[i].REF,mutect_df_list[i].ALT))"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wYborCIAHrcD"
      },
      "source": [
        "#check if the adding columns worked\n",
        "for i in mutect_df_list:\n",
        "  i.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "karoXyN_YESL"
      },
      "source": [
        "These columns now can be taken and used for plotting in excel (doing stacked percent bar charts in python is weirdly difficult)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OuvFQSrGH3qT"
      },
      "source": [
        "# Count all occurences of each type of mutation and \n",
        "# display the resulting summary table for each DF\n",
        "for i in mutect_df_list:\n",
        "  i.groupBy(\"Type\").count().show()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}