{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Sani.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TqrXHlb8mmOV"
      },
      "source": [
        "# Variant Analysis\n",
        "\n",
        "This Jupyter notebook is designed for filtration of common variants and analysis as outlibed by the steps in my [github repository](https://github.com/Intro-Sci-Comp-UIowa/biol-4386-course-project-tvarovski). This notebook will be using spark distributred computing environment for faster computation."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wXGbbaHq8tsq"
      },
      "source": [
        "## Installing Dependencies and Importing Libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m1VMVSpOmKlJ"
      },
      "source": [
        "!apt-get update\n",
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
        "!wget -q https://www-us.apache.org/dist/spark/spark-2.4.7/spark-2.4.7-bin-hadoop2.7.tgz\n",
        "!tar xf spark-2.4.7-bin-hadoop2.7.tgz\n",
        "!pip install -q findspark\n",
        "\n",
        "import os\n",
        "import findspark\n",
        "import pandas as pd\n",
        "\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-2.4.7-bin-hadoop2.7\"\n",
        "findspark.init()\n",
        "\n",
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder.appName(\"CLC_mutations\").getOrCreate()\n",
        "\n",
        "!pip install scikit-allel[full]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O39d6UPMmQn2"
      },
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "import functools\n",
        "from pyspark.sql.functions import lit\n",
        "import allel"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cktC6SCcmfJu"
      },
      "source": [
        "## Loading Datasets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-Conwr2xmT2J"
      },
      "source": [
        "#If the vxf file is not converted into a a \"normal\" table do it here\n",
        "\n",
        "fields=['CHROM', 'POS', 'DP','AD', 'AF', 'REF', 'ALT','INFO', 'ID']\n",
        "\n",
        "#allel.vcf_to_csv('.vcf', 'mutect_07.csv', fields=fields)\n",
        "#allel.vcf_to_csv('.vcf', 'mutect_17.csv', fields=fields)\n",
        "allel.vcf_to_csv('/content/SRR4047722_mutect.vcf', 'mutect_22.csv', fields=fields)\n",
        "allel.vcf_to_csv('/content/SRR4047723_mutect.vcf', 'mutect_23.csv', fields=fields)\n",
        "\n",
        "#allel.vcf_to_csv('.vcf', 'haplotype_07.csv', fields=fields)\n",
        "#allel.vcf_to_csv('.vcf', 'haplotype_15.csv', fields=fields)\n",
        "#allel.vcf_to_csv('.vcf', 'haplotype_17.csv', fields=fields)\n",
        "#allel.vcf_to_csv('.vcf', 'haplotype_22.csv', fields=fields)\n",
        "#allel.vcf_to_csv('.vcf', 'haplotype_23.csv', fields=fields)\n"
      ],
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3UBDN509HW2Z"
      },
      "source": [
        "Load csv files into Data Frames; insert DF objects to lists"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E-GHKh8mHefU"
      },
      "source": [
        "#df_mutect_07 = spark.read.csv('mutect_07.csv', header=True, inferSchema=True)\n",
        "#df_mutect_17 = spark.read.csv('mutect_17.csv', header=True, inferSchema=True)\n",
        "df_mutect_22 = spark.read.csv('out22.tsv', header=True, inferSchema=True, sep='\\t')\n",
        "df_mutect_23 = spark.read.csv('out23.tsv', header=True, inferSchema=True, sep='\\t')\n",
        "\n",
        "#df_haplotype_07 = spark.read.csv('haplotype_07.csv', header=True, inferSchema=True)\n",
        "#df_haplotype_15 = spark.read.csv('haplotype_15.csv', header=True, inferSchema=True)\n",
        "#df_haplotype_17 = spark.read.csv('haplotype_17.csv', header=True, inferSchema=True)\n",
        "#df_haplotype_22 = spark.read.csv('haplotype_22.csv', header=True, inferSchema=True)\n",
        "#df_haplotype_23 = spark.read.csv('haplotype_23.csv', header=True, inferSchema=True)\n",
        "\n",
        "mutect_df_list = [df_mutect_22, df_mutect_23] #df_mutect_07, df_mutect_17, \n",
        "#haplotype_df_list = [df_haplotype_07, df_haplotype_15, df_haplotype_17, df_haplotype_22, df_haplotype_23]"
      ],
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ywele90Ana47"
      },
      "source": [
        "#describing All Variant Stats to get an idea what we have\n",
        "for i in mutect_df_list:\n",
        "  #first show first few rows\n",
        "  i.show()\n",
        "  #describe and show basic stats\n",
        "  i.describe().show()\n",
        "\n",
        "#for i in haplotype_df_list:\n",
        "  #i.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c8lFTmunuAhd"
      },
      "source": [
        "## Preparing DataFrames for Filtering"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FjPZSBoM-8om"
      },
      "source": [
        "Select and Rename DataFrame Columns"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "52sUZ-IPnkyr"
      },
      "source": [
        "fields=['CHROM', 'POS', 'TYPE','REF']\n",
        "\n",
        "# cut out the normal columns (I think they are not relevant)\n",
        "condition = lambda col: 'h4.lib1' in col\n",
        "\n",
        "# rename sample columns so that they are consistent across all samples\n",
        "renaming_AF = lambda col: 'AF' in col\n",
        "renaming_AD = lambda col: 'AD' in col\n",
        "\n",
        "for i in range(len(mutect_df_list)):\n",
        "  mutect_df_list[i] = mutect_df_list[i].drop(*filter(condition, mutect_df_list[i].columns))\n",
        "  mutect_df_list[i] = mutect_df_list[i].withColumnRenamed(*filter(renaming_AF, mutect_df_list[i].columns), 'AF')\n",
        "  mutect_df_list[i] = mutect_df_list[i].withColumnRenamed(*filter(renaming_AD, mutect_df_list[i].columns), 'AD')\n",
        "  #mutect_df_list[i] = mutect_df_list[i].select(fields)\n",
        "  \n",
        "\n",
        "#for i in range(len(haplotype_df_list)):\n",
        "#  df_list[i] = df_list[i].select(fields)"
      ],
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gXqhVk9c-bJc"
      },
      "source": [
        "#describing All Variant Stats to check if renaming worked\n",
        "\n",
        "for i in mutect_df_list:\n",
        "  #first show first few rows\n",
        "  i.show()\n",
        "\n",
        "#for i in haplotype_df_list:\n",
        "  #i.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t_bn4VX4nt6U"
      },
      "source": [
        "## Quality and SNP Filtering"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_F94PyGLnvw9",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        },
        "outputId": "2e1b5234-9224-4490-f1a4-60e9556c8d30"
      },
      "source": [
        "for i in range(len(mutect_df_list)):\n",
        "  mutect_df_list[i] = mutect_df_list[i].filter(\n",
        "               mutect_df_list[i].TYPE == \"SNP\").filter(\n",
        "               (mutect_df_list[i].AF >= 0.45) & (mutect_df_list[i].AF <= 0.55) | (mutect_df_list[i].AF >= 0.90))\n",
        "\n",
        "'''for i in range(len(haplotype_df_list)):\n",
        "  df_list[i] = df_list[i].filter(\n",
        "               df_list[i].Frequency >= 45).filter(\n",
        "               df_list[i][\"Average quality\"] >= 20).filter(\n",
        "               df_list[i].Coverage >= 10).filter(\n",
        "               df_list[i].Count >= 4).filter(\n",
        "               df_list[i].Coverage <= 100).filter(\n",
        "               df_list[i].Type == \"SNV\")'''\n"
      ],
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'for i in range(len(haplotype_df_list)):\\n  df_list[i] = df_list[i].filter(\\n               df_list[i].Frequency >= 45).filter(\\n               df_list[i][\"Average quality\"] >= 20).filter(\\n               df_list[i].Coverage >= 10).filter(\\n               df_list[i].Count >= 4).filter(\\n               df_list[i].Coverage <= 100).filter(\\n               df_list[i].Type == \"SNV\")'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 64
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2Y_J_ajpoA1R"
      },
      "source": [
        "## Subtraction of Common Variants Between Samples\n",
        "\n",
        "Maybe should do it by position only?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hmTzg3gVn_Yb"
      },
      "source": [
        "subtracted_df_list = []\n",
        "\n",
        "for i in df_list:\n",
        "  for j in df_list:\n",
        "    temp_df = i\n",
        "    if i != j:\n",
        "      temp_df = temp_df.subtract(j)\n",
        "\n",
        "  subtracted_df_list.append(temp_df)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DO2A_PnSBQa8"
      },
      "source": [
        "## Preparing the Data for plotting"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TqLmUPhKBNo4"
      },
      "source": [
        "from pyspark.sql.types import StringType\n",
        "from pyspark.sql.functions import udf, struct\n",
        "\n",
        "def findType(colA, colB):\n",
        "  if ((colA == 'C') & (colB == 'T') | (colA == 'G') & (colB == 'A')):\n",
        "    return('C_to_T')\n",
        "  if ((colA == 'C') & (colB == 'A') | (colA == 'G') & (colB == 'T')):\n",
        "    return('C_to_A')\n",
        "  if ((colA == 'C') & (colB == 'G') | (colA == 'G') & (colB == 'C')):\n",
        "    return('C_to_G')\n",
        "  if ((colA == 'T') & (colB == 'C') | (colA == 'A') & (colB == 'G')):\n",
        "    return('T_to_C')\n",
        "  if ((colA == 'T') & (colB == 'G') | (colA == 'A') & (colB == 'C')):\n",
        "    return('T_to_G')\n",
        "  if ((colA == 'T') & (colB == 'A') | (colA == 'A') & (colB == 'T')):\n",
        "    return('T_to_A')\n",
        "  \n",
        "createType = udf(findType, StringType())\n",
        "\n",
        "for i in range(len(mutect_df_list)):\n",
        "  mutect_df_list[i] = mutect_df_list[i].withColumn('Type', createType(mutect_df_list[i].REF,mutect_df_list[i].ALT))"
      ],
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wYborCIAHrcD"
      },
      "source": [
        "  #check if the adding columns worked\n",
        "for i in mutect_df_list:\n",
        "  i.show()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "karoXyN_YESL"
      },
      "source": [
        "These columns now can be taken and used for plotting in excel (doing stacked percent bar charts in python is weirdly difficult)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OuvFQSrGH3qT"
      },
      "source": [
        "for i in mutect_df_list:\n",
        "  i.groupBy(\"Type\").count().show()"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}